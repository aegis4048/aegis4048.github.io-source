Title: Demystifying Neural Network in Skip-Gram Language Modeling
Tags: data-mining, nlp, word2vec, co-occurrence matrix, vector space model, word vectors, window size, skip-gram, neural network, negative sampling, stochastic gradient descent, learning rate
Date: 2019-05-06 09:00
Slug: demystifying_neural_network_in_skip_gram_language_modeling
Subtitle:
Keywords: 
Featured_Image: images/featured_images/skip-gram.png
readingTime: 16
Social_Media_Description: How is neural network used in language modeling to capture relationships among words?
IndexPreview: The past couple of years, neural networks in Word2Vec have nearly taken over the field of NLP, thanks to their state-of-art performance. But how much do you understand about the algorithm behind it? This post will crack the secrets behind neural net in Word2Vec.
Summary: {% notebook downloads/notebooks/demystifying_neural_net_algorithm_behind_word2vec_skip_gram.ipynb cells[1:2] %}

{% notebook downloads/notebooks/demystifying_neural_net_algorithm_behind_word2vec_skip_gram.ipynb cells[:] %}