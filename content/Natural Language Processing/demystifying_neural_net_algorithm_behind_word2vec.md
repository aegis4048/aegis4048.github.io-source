Title: Demystifying Neural Network in Skip-Gram Language Modeling
Tags: data-mining, nlp, word2vec, co-occurrence matrix, vector space model, word vectors, window size, skip-gram, neural network, negative sampling, stochastic gradient descent, learning rate
Date: 2019-05-06 09:00
Slug: demystifying_neural_network_algorithm_behind_word2vec_skip_gram
Subtitle:
Keywords: 
Featured_Image: images/featured_images/skip-gram.png
readingTime: 16
Social_Media_Description: How is neural network used in language modeling to capture relationships among words?
IndexPreview: Thanks to their state-of-art performance, neural networks in Word2Vec have nearly taken over the field of NLP the past couple of years. But how much do you understand about the algorithm behind it? This post will crack the secrets behind neural network in NLP's.
Summary: {% notebook downloads/notebooks/demystifying_neural_net_algorithm_behind_word2vec_skip_gram.ipynb cells[1:2] %}

{% notebook downloads/notebooks/demystifying_neural_net_algorithm_behind_word2vec_skip_gram.ipynb cells[:] %}
****