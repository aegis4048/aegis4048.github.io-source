Title: Optimize Computational Efficiency in Skip-Gram with Negative Sampling
Tags: data-mining, nlp, word2vec, co-occurrence matrix, vector space model, word vectors, window size, skip-gram, neural network, negative sampling, stochastic gradient descent, learning rate
Date: 2019-05-24 09:00
Slug: optimizing_computational_efficiency_in_skip-gram_with_negative_sampling
Subtitle:
Keywords: 
Featured_Image: images/featured_images/skip-gram.png
readingTime: 20
Social_Media_Description: How is neural network used in language modeling to capture relationships among words?
IndexPreview: Negative sampling
Summary: {% notebook downloads/notebooks/Optimizing_Computational_Efficiency_in_Skip-Gram_with_Negative_Sampling.ipynb cells[1:2] %}

{% notebook downloads/notebooks/Optimizing_Computational_Efficiency_in_Skip-Gram_with_Negative_Sampling.ipynb cells[:] %}