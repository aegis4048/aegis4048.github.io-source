Title: Optimize Computational Efficiency of Skip-Gram with Negative Sampling
Tags: data-mining, nlp, word2vec, word vectors, window size, skip-gram, neural network, negative sampling, stochastic gradient descent, learning rate, sigmoid, softmax, algorithm complexity, noise distribution 
Date: 2019-05-26
DatePrev: 2019-05-26
Slug: optimize_computational_efficiency_of_skip-gram_with_negative_sampling
Subtitle:
Keywords: 
Featured_Image: images/featured_images/negative_sampling.png
readingTime: 22
Social_Media_Description: How is negative sampling used to improve computational efficieny of Word2Vec?
IndexPreview: When training your NLP model with Skip-Gram, the very large size of vocabs imposes high computational cost on your machine. Since the original Skip-Gram model is unable to handle this high cost, we use an alternative, called Negative Sampling. 
Summary: {% notebook downloads/notebooks/Optimizing_Computational_Efficiency_of_Skip-Gram_with_Negative_Sampling.ipynb cells[1:2] %}

{% notebook downloads/notebooks/Optimizing_Computational_Efficiency_of_Skip-Gram_with_Negative_Sampling.ipynb cells[:] %}